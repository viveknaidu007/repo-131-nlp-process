{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"1.Text Cleaning:\n",
    "\n",
    "    Lowercasing: Convert all characters to lowercase to maintain uniformity.\n",
    "    Removing Punctuation: Remove punctuation marks to avoid treating them as separate words.\n",
    "    Removing Special Characters: Eliminate special characters (e.g., @, #, $).\n",
    "    Removing Numbers: Depending on the task, numbers may be removed.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"2.Tokenization:\n",
    "\n",
    "    Split the text into individual words or tokens. For example, \"Hello world!\" becomes [\"Hello\", \"world\"].\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3.Removing Stop Words:\n",
    "\n",
    "    Remove common words that do not contribute much meaning, such as \"and\", \"the\", \"is\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"4.Stemming:\n",
    "\n",
    "    Reduce words to their base or root form. For example, \"running\" becomes \"run\". Common stemming algorithms include Porter Stemmer and Snowball Stemmer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"5.Lemmatization:\n",
    "\n",
    "    Reduce words to their dictionary form, taking into account the context. For example, \"better\" becomes \"good\". Common lemmatization tools include WordNet Lemmatizer.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"6.POS Tagging (Part-of-Speech Tagging):\n",
    "\n",
    "    Assign part-of-speech tags to each word (e.g., noun, verb, adjective). This can be useful for understanding the grammatical structure of the text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"7.Named Entity Recognition (NER):\n",
    "\n",
    "    Identify and classify named entities (e.g., person names, organizations, locations) within the text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"8.Removing Noise:\n",
    "\n",
    "    Eliminate any irrelevant information or noise specific to your dataset. For example, in a dataset of tweets, you might remove URLs and mentions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"9.Handling Negations:\n",
    "\n",
    "    Address negations in text to understand the true sentiment. For example, \"not happy\" should be treated differently from \"happy\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"10.Spelling Correction:\n",
    "\n",
    "    Correct spelling errors to improve the quality of the text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"11.Text Normalization:\n",
    "\n",
    "    Standardize words to a common format. For example, converting abbreviations to their full form (\"u\" to \"you\").\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"12.Vectorization:\n",
    "\n",
    "    Convert text into numerical format for machine learning models. Common methods include:\n",
    "        Bag of Words (BoW): Represents text as a frequency distribution of words.\n",
    "        TF-IDF (Term Frequency-Inverse Document Frequency): Weighs the importance of words based on their frequency in a document and across all documents.\n",
    "        Word Embeddings: Converts text into dense vectors capturing semantic meaning. Popular models include Word2Vec, GloVe, and FastText.\n",
    "        Contextual Embeddings: Use models like BERT, GPT-3 that capture context-based representations of words.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello world! This is a test message. NLP is fun.\"\n",
    "\n",
    "# Step 1: Text Cleaning\n",
    "text = text.lower()\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Step 2: Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Step 3: Removing Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Step 4: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Step 5: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# For POS Tagging and NER using spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" \".join(tokens))\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "# Named Entity Recognition\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "\n",
    "print(\"Cleaned Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n",
    "print(\"Named Entities:\", entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy<2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
